# team_bereza

Локальный RAG-пайплайн для генерации клинических рекомендаций по онкологии.
Принимает на вход PDF клинических руководств и DOCX-карту пациента, возвращает структурированный ответ в двух режимах: для врача и для пациента.

---

## Архитектура

```
PDF (клинические рекомендации)
    → OCR / native extract   [src/ocr.py]
    → Semantic chunker        [src/parser.py]
    → TextEmbedder            [src/embeddings.py]
    → FaissStore (кэш)        [src/faiss_store.py]
                  ↕ top-k retrieval
DOCX (данные пациента)
    → TextEmbedder
    → LocalGenerator          [src/generator.py]
    → version_N.json          [data/generator_response/]
```

Пайплайн полностью локальный — никаких внешних API.

---

## Установка

```bash
pip install -r requirements.txt
```

> Torch устанавливается с CUDA 12.1. Для CPU-only замени строку в `requirements.txt` на `torch==2.5.0`.

---

## Запуск

```bash
python main.py
```

Все параметры задаются в `src/config.py`:

```python
from src.config import Config

cfg = Config(
    mode="doctor",          # "doctor" | "patient"
    device="cuda",          # "cpu" | "cuda"
    top_k=5,
    max_new_tokens=1024,
)
```

При первом запуске пайплайн выполняет OCR, чанкинг и строит FAISS-индекс, сохраняя его в `data/faiss_index/`. При последующих запусках индекс загружается с диска — OCR и эмбеддинг гайдлайнов пропускаются.

Чтобы пересобрать индекс (например, после добавления нового PDF):

```bash
rm -rf data/faiss_index/
python main.py
```

---

## Тестирование

Jupyter-ноутбук `tests.ipynb` в корне проекта содержит изолированные тесты каждого модуля и полный end-to-end прогон.

```bash
jupyter notebook tests.ipynb
```

---

## Структура проекта

```
team_bereza/
├── main.py                         # точка входа, линейный пайплайн
├── tests.ipynb                     # тесты всех модулей
├── requirements.txt
├── src/
│   ├── config.py                   # централизованная конфигурация
│   ├── ocr.py                      # извлечение текста из PDF
│   ├── parser.py                   # семантический чанкер
│   ├── embeddings.py               # TextEmbedder (sentence-transformers)
│   ├── faiss_store.py              # векторное хранилище
│   └── generator.py                # LocalGenerator (Qwen2-7B-Instruct)
└── data/
    ├── clinical_guideline/         # PDF клинических рекомендаций
    ├── input_example/              # DOCX пациента
    ├── faiss_index/                # кэш индекса (генерируется автоматически)
    └── generator_response/         # версионированные JSON-ответы
```

---

## Дизайн решения

### Компоненты

| Компонент | Класс / функция | Модель / библиотека |
|-----------|----------------|---------------------|
| OCR / извлечение текста | `extract_text_from_pdf()` | PyMuPDF + EasyOCR |
| Чанкер | `GuidelineParserStub` | tokenizer all-MiniLM-L6-v2 |
| Эмбеддер | `TextEmbedder` | sentence-transformers/all-MiniLM-L6-v2 |
| Векторное хранилище | `FaissStore` | faiss.IndexFlatL2 |
| Генератор | `LocalGenerator` | Qwen2-7B-Instruct |

### Полный поток данных

```
ВХОДНЫЕ ДАННЫЕ
├── PDF (клинические рекомендации)
└── DOCX (карта пациента)
         │
         ▼
┌─────────────────────────────────────────────┐
│  ВЕТКА А: FAISS-индекс                      │
│                                             │
│  Проверка: index.faiss + texts.pkl          │
│           + metadata.pkl существуют?        │
│                                             │
│  ДА ──────────────────► загрузить индекс    │
│                          с диска            │
│                                             │
│  НЕТ                                        │
│   │                                         │
│   ▼                                         │
│  PDF → extract_text_from_pdf()              │
│   │                                         │
│   │  Детекция типа PDF:                     │
│   ├─ есть текстовый слой → PyMuPDF          │
│   └─ только картинки   → EasyOCR            │
│             │                               │
│             ▼                               │
│        сырой текст (с \n\n абзацами)        │
│             │                               │
│             ▼                               │
│  GuidelineParserStub.parse()                │
│   1. split по \n\n → абзацы                 │
│   2. группировка мелких абзацев             │
│      до chunk_size=500 токенов              │
│   3. крупные абзацы → токен-окна            │
│      с overlap=50                           │
│             │                               │
│             ▼                               │
│        list[str] — чанки                   │
│             │                               │
│             ▼                               │
│  TextEmbedder.embed_batch()                 │
│  → ndarray (N, 384), L2-нормализованные     │
│             │                               │
│             ▼                               │
│  FaissStore.add() → сохранить на диск       │
└─────────────────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────────┐
│  ВЕТКА Б: ПАЦИЕНТ                           │
│                                             │
│  DOCX → load_patient_docx()                 │
│       → str (текст карты пациента)          │
│             │                               │
│             ▼                               │
│  TextEmbedder.embed_text()                  │
│  → ndarray (1, 384), L2-нормализованный     │
└─────────────────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────────┐
│  RETRIEVAL                                  │
│                                             │
│  FaissStore.search(query, top_k=5)          │
│  → 5 ближайших чанков (L2 = cosine dist)   │
└─────────────────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────────┐
│  ГЕНЕРАЦИЯ                                  │
│                                             │
│  LocalGenerator.generate()                 │
│                                             │
│  apply_chat_template:                       │
│  ┌─ system: промпт режима                  │
│  └─ user:  [чанки] + [карта пациента]      │
│             │                               │
│             ▼                               │
│  Qwen2-7B-Instruct                          │
│  do_sample=False, max_new_tokens=1024       │
│             │                               │
│             ▼                               │
│  decode(new_tokens_only) → str             │
└─────────────────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────────┐
│  ВЫХОД                                      │
│                                             │
│  version_N.json:                            │
│  ├── final_response                         │
│  ├── mode (doctor / patient)                │
│  ├── model_name                             │
│  ├── stage_durations                        │
│  └── timestamp_start / end                 │
└─────────────────────────────────────────────┘
```

### Ключевые развилки

**1. При старте пайплайна:**
- `index.faiss` существует? → **да** → загрузить → пропустить OCR / чанкинг / эмбеддинг гайдлайнов
- → **нет** → OCR → чанкинг → эмбеддинг → сохранить индекс

**2. Внутри OCR:**
- PDF имеет текстовый слой? → **да** → PyMuPDF (мгновенно)
- → **нет** → EasyOCR (рендер страниц в 300 DPI → numpy → OCR)

**3. Внутри чанкера (для каждого абзаца):**
- абзац > 500 токенов? → **да** → токен-окна с overlap
- накопленный буфер + абзац > 500 токенов? → **да** → flush буфера, начать новый
- → **нет** → добавить абзац в буфер

**4. Флаг `add_new_guidelines`:**
- `True` и индекс существует и новый PDF существует → допиндексировать новый PDF и сохранить обновлённый индекс

### Что не входит в основную схему

- Валидации пустого текста и размерностей — guard-клозы внутри модулей, не ветки бизнес-логики
- Версионирование JSON — линейная запись после генерации, отдельной ветки нет
- `GuidelineParser` (LLM-парсер) — реализован в `parser.py`, но в пайплайн не включён

---

## История версий

| Версия | Дата       | Изменения |
|--------|------------|-----------|
| v0.1   | 2026-02-20 | Минимальная работоспособная версия RAG |
| v0.2   | 2026-02-21 | Добавлена логика режимов ответов: doctor / patient |
| v0.3   | 2026-02-22 | Рефакторинг и улучшение всех модулей (см. ниже) |

---

## Изменения v0.3

### `src/ocr.py` — замена Tesseract на EasyOCR + PyMuPDF

**Было:** `pytesseract` + `pdf2image` с захардкоженным путём к `tesseract.exe`.
**Стало:** два бэкенда в одной функции `extract_text_from_pdf()`:

- **Native PDF** (есть текстовый слой) → `PyMuPDF` извлекает текст напрямую, мгновенно.
- **Scanned PDF** (нет текстового слоя) → страницы рендерятся через `PyMuPDF` в 300-DPI numpy-массивы и передаются в `EasyOCR`.

EasyOCR Reader кэшируется в памяти (`_READER_CACHE`) — при повторных вызовах веса не перезагружаются.
Нормализация теперь **сохраняет структуру абзацев** (`\n\n`), а не схлопывает весь текст в одну строку.
Удалены зависимости: `pytesseract`, `pdf2image`, системный Tesseract.

---

### `src/parser.py` — семантический чанкер

**Было:** `GuidelineParserStub` — простой токен-слайсинг с фиксированным шагом, без учёта структуры текста.
**Стало:** трёхуровневый алгоритм в `GuidelineParserStub.parse()`:

1. Разбивка по `\n\n` (абзацы, сохранённые новым OCR).
2. Накопление мелких абзацев в один чанк, пока не превышен `chunk_size`.
3. Для одиночных абзацев крупнее `chunk_size` — токен-окна с `overlap` (прежнее поведение).

Результат: чанки семантически связны, не режут предложения посередине там, где это не нужно.

---

### `src/generator.py` — chat template

**Было:** системный промпт и данные конкатенировались в сырую f-строку, затем промпт вручную срезался из ответа (`startswith` + слайс).
**Стало:**

- Системные промпты вынесены в константу класса `_SYSTEM_PROMPTS` — один dict на оба режима.
- `_get_system_prompt(mode)` выбрасывает явный `ValueError` при неизвестном режиме.
- Форматирование через `tokenizer.apply_chat_template()` — правильный формат для Qwen2-7B-Instruct (и любой другой chat-модели с шаблоном).
- Декодируются **только новые токены** (`output_ids[0][input_length:]`) — хак со срезанием промпта из ответа удалён.

---

### `src/config.py` — новый файл

Датакласс `Config` с дефолтами для всех параметров пайплайна: пути к файлам, имена моделей, языки OCR, `chunk_size`, `overlap`, `top_k`, `max_new_tokens`, `device`, `mode`.
`main.py` теперь содержит только `cfg = Config()` — никаких разбросанных констант.

---

### `tests.ipynb` — новый файл

Jupyter-ноутбук с изолированными тестами каждого модуля:
- автоопределение CUDA и `DEVICE`
- проверка всех зависимостей
- OCR с замером времени и статистикой по абзацам
- parser: юнит-тест на синтетическом тексте + реальный PDF
- embedder: проверка L2-нормализации, матрица косинусного сходства
- FAISS: add / search / save / load
- generator: проверка chat template, генерация в обоих режимах
- полный пайплайн через `main()` + просмотр `version_N.json`

---

### `requirements.txt`

| Было | Стало |
|------|-------|
| `pytesseract` | удалено |
| `pdf2image` | удалено |
| `Pillow==11.0.0` | `Pillow` (без пина) |
| — | `easyocr` |
| — | `PyMuPDF` |
