from __future__ import annotations

import json
import logging
import re
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

logger = logging.getLogger(__name__)


class LocalGenerator:
    """
    Generate clinical recommendations using a local HuggingFace chat model.

    This class:
    - Loads a causal language model locally (no external APIs).
    - Formats prompts via ``tokenizer.apply_chat_template`` for proper
      instruction-following with chat models such as Qwen2-7B-Instruct.
    - Supports two generation modes: ``"doctor"`` and ``"patient"``.
    - Returns only the newly generated tokens (no prompt stripping required).
    """

    _DISCLAIMER = (
        "\n\n–ù–µ —è–≤–ª—è–µ—Ç—Å—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–µ–π! "
        "–ú–∞—Ç–µ—Ä–∏–∞–ª —Å–æ–∑–¥–∞–Ω –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤ –æ–∑–Ω–∞–∫–æ–º–∏—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–ª—è—Ö."
    )

    _SYSTEM_PROMPTS: dict[str, str] = {
        "doctor": (
            "–¢—ã ‚Äî –≤—Ä–∞—á-–æ–Ω–∫–æ–ª–æ–≥. –û—Ü–µ–Ω–∏–≤–∞–π —Ç–∞–∫—Ç–∏–∫—É –ª–µ—á–µ–Ω–∏—è –ø–∞—Ü–∏–µ–Ω—Ç–∞.\n\n"
            "üî¥ –ì–õ–ê–í–ù–û–ï –ü–†–ê–í–ò–õ–û ‚Äî –ò–°–¢–û–ß–ù–ò–ö –î–ê–ù–ù–´–•:\n"
            "–¢–≤–æ–π –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∑–Ω–∞–Ω–∏–π ‚Äî –±–ª–æ–∫ ¬´–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏¬ª –≤ –∑–∞–ø—Ä–æ—Å–µ "
            "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ö–∞–∂–¥—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –ø–æ–º–µ—á–µ–Ω —Ç–µ–≥–æ–º [–ö–†: –Ω–∞–∑–≤–∞–Ω–∏–µ]. "
            "–ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ. –í—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ ‚Äî –∑–∞–ø—Ä–µ—â–µ–Ω–æ.\n\n"
            "üî¥ –°–¢–†–û–ì–û –ó–ê–ü–†–ï–©–ï–ù–û:\n"
            "‚Äî –Ω–∞–∑—ã–≤–∞—Ç—å –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã, —Å—Ö–µ–º—ã, –¥–æ–∑—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Ç–µ–≥–∞—Ö [–ö–†: ...];\n"
            "‚Äî —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É, —É—Ä–æ–≤–Ω–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –≤—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç—å ‚Äî –µ—Å–ª–∏ –æ–Ω–∏ "
            "–Ω–µ –ø—Ä–æ—Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω—ã –¥–æ—Å–ª–æ–≤–Ω–æ –∏–∑ [–ö–†: ...];\n"
            "‚Äî —É–ø–æ–º–∏–Ω–∞—Ç—å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ (RUSSCO, ESMO, NCCN –∏ –¥—Ä.) ‚Äî –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –Ω–∞–∑–≤–∞–Ω—ã "
            "–≤ —Ç–µ–∫—Å—Ç–µ [–ö–†: ...];\n"
            "‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–Ω–∞–Ω–∏—è –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –ª–∏—á–Ω—ã–π –æ–ø—ã—Ç;\n"
            "‚Äî –ø—Ä–∏–¥—É–º—ã–≤–∞—Ç—å –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–ª–∏ —Å–ª–æ–≤–∞.\n\n"
            "–ï—Å–ª–∏ –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö [–ö–†: ...] –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É ‚Äî –Ω–∞–ø–∏—à–∏:\n"
            "¬´–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ö –ö–† –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –ø—É–Ω–∫—Ç—É –Ω–µ—Ç.¬ª\n\n"
            "‚õî –¢–ï–ú–ê:\n"
            "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –æ–Ω–∫–æ–ª–æ–≥–∏–∏ –∏–ª–∏ –ª–µ—á–µ–Ω–∏—é —ç—Ç–æ–≥–æ –ø–∞—Ü–∏–µ–Ω—Ç–∞ ‚Äî –æ—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ:\n"
            "¬´–Ø —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ –æ–Ω–∫–æ–ª–æ–≥–∏–∏ –∏ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö. "
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ —Ç–µ–º–µ –ª–µ—á–µ–Ω–∏—è –ø–∞—Ü–∏–µ–Ω—Ç–∞.¬ª\n\n"
            "üîπ –°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê:\n"
            "1. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º ‚Äî 2‚Äì4 —Ç–µ–∑–∏—Å–∞, –∫–∞–∂–¥—ã–π —Å–æ —Å—Å—ã–ª–∫–æ–π "
            "–≤–∏–¥–∞ (–ö–†: –Ω–∞–∑–≤–∞–Ω–∏–µ).\n"
            "2. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–µ—Ä–∞–ø–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º ‚Äî 2‚Äì5 —Ç–µ–∑–∏—Å–æ–≤ —Å–æ —Å—Å—ã–ª–∫–∞–º–∏.\n"
            "3. –û—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –∏–ª–∏ —Å–ø–æ—Ä–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã ‚Äî –µ—Å–ª–∏ –µ—Å—Ç—å, –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –∏ –±–µ–∑ –¥–æ–º—ã—Å–ª–æ–≤.\n"
            "4. –ß—Ç–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–∞–ª–µ–µ ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –ø—Ä—è–º–æ —Å–ª–µ–¥—É–µ—Ç –∏–∑ [–ö–†: ...].\n\n"
            "–°—Ç–∏–ª—å: –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –∫–æ–ª–ª–µ–≥–∞‚Äì–∫–æ–ª–ª–µ–≥–µ. –ë–µ–∑ –ø–µ—Ä–µ—Å–∫–∞–∑–∞ –∏—Å—Ç–æ—Ä–∏–∏ –±–æ–ª–µ–∑–Ω–∏. "
            "–ë–µ–∑ –ø–æ–≤—Ç–æ—Ä–æ–≤. –ë–µ–∑ —Ç–∞–±–ª–∏—Ü –∏ JSON.\n\n"
            "–í –∫–æ–Ω—Ü–µ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –¥–æ–±–∞–≤—å:\n"
            "–ù–µ —è–≤–ª—è–µ—Ç—Å—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–µ–π! –ú–∞—Ç–µ—Ä–∏–∞–ª —Å–æ–∑–¥–∞–Ω –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é. "
            "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤ –æ–∑–Ω–∞–∫–æ–º–∏—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–ª—è—Ö."
        ),
        "patient": (
            "–¢—ã ‚Äî –≤—Ä–∞—á-–æ–Ω–∫–æ–ª–æ–≥, –æ–±—ä—è—Å–Ω—è—é—â–∏–π –ø–∞—Ü–∏–µ–Ω—Ç—É –µ–≥–æ —Å–∏—Ç—É–∞—Ü–∏—é.\n\n"
            "üî¥ –ì–õ–ê–í–ù–û–ï –ü–†–ê–í–ò–õ–û ‚Äî –ò–°–¢–û–ß–ù–ò–ö –î–ê–ù–ù–´–•:\n"
            "–¢–≤–æ–π –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∑–Ω–∞–Ω–∏–π ‚Äî –±–ª–æ–∫ ¬´–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏¬ª –≤ –∑–∞–ø—Ä–æ—Å–µ. "
            "–ö–∞–∂–¥—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –ø–æ–º–µ—á–µ–Ω —Ç–µ–≥–æ–º [–ö–†: –Ω–∞–∑–≤–∞–Ω–∏–µ]. "
            "–ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ. –í—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ ‚Äî –∑–∞–ø—Ä–µ—â–µ–Ω–æ.\n\n"
            "üî¥ –°–¢–†–û–ì–û –ó–ê–ü–†–ï–©–ï–ù–û:\n"
            "‚Äî –Ω–∞–∑—ã–≤–∞—Ç—å –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã –∏–ª–∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Ç–µ–≥–∞—Ö [–ö–†: ...];\n"
            "‚Äî –≤—ã–¥—É–º—ã–≤–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏ –ø—Ä–æ–≥–Ω–æ–∑—ã;\n"
            "‚Äî —É–ø–æ–º–∏–Ω–∞—Ç—å –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ [–ö–†: ...];\n"
            "‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–Ω–∞–Ω–∏—è –∏–∑ –æ–±—É—á–µ–Ω–∏—è.\n\n"
            "–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏: ¬´–ü–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É –≤ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–Ω–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö "
            "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç.¬ª\n\n"
            "‚õî –¢–ï–ú–ê: —Ç–æ–ª—å–∫–æ –æ–Ω–∫–æ–ª–æ–≥–∏—è –∏ –ª–µ—á–µ–Ω–∏–µ —ç—Ç–æ–≥–æ –ø–∞—Ü–∏–µ–Ω—Ç–∞.\n\n"
            "üîπ –ö–ê–ö –û–¢–í–ï–ß–ê–¢–¨:\n"
            "1. –ö—Ä–∞—Ç–∫–æ –∏ –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏: –¥–∏–∞–≥–Ω–æ–∑, —á—Ç–æ —É–∂–µ —Å–¥–µ–ª–∞–Ω–æ, —Ç–µ–∫—É—â–∞—è —Å–∏—Ç—É–∞—Ü–∏—è.\n"
            "2. –ü–æ—á–µ–º—É –≤—ã–±—Ä–∞–Ω–∞ —ç—Ç–∞ —Ç–∞–∫—Ç–∏–∫–∞ ‚Äî —Å–æ —Å—Å—ã–ª–∫–æ–π (–ö–†: –Ω–∞–∑–≤–∞–Ω–∏–µ).\n"
            "3. –ß—Ç–æ –¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ —Å–ª–µ–¥—É–µ—Ç –∏–∑ [–ö–†: ...].\n"
            "4. –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ ‚Äî –º—è–≥–∫–æ, –±–µ–∑ –æ–±–≤–∏–Ω–µ–Ω–∏–π –≤—Ä–∞—á–∞.\n\n"
            "–ì–æ–≤–æ—Ä–∏ –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫, –Ω–µ –∫–∞–∫ –ø—Ä–æ—Ç–æ–∫–æ–ª. –ë–µ–∑ –ª–∞—Ç—ã–Ω–∏ –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è. "
            "–ë–µ–∑ –ø–∞–Ω–∏–∫–∏. –ú–∏–Ω–∏–º—É–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π.\n\n"
            "–í –∫–æ–Ω—Ü–µ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –¥–æ–±–∞–≤—å:\n"
            "–ù–µ —è–≤–ª—è–µ—Ç—Å—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–µ–π! –ú–∞—Ç–µ—Ä–∏–∞–ª —Å–æ–∑–¥–∞–Ω –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é. "
            "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤ –æ–∑–Ω–∞–∫–æ–º–∏—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–ª—è—Ö."
        ),
    }

    def __init__(self, model_name: str, device: str = "cpu") -> None:
        """
        Initialize the local generator with a HuggingFace chat model.

        Args:
            model_name: Name or path of the HuggingFace causal language model.
            device: Device identifier, e.g. ``"cpu"`` or ``"cuda"``.
        """
        self.model_name = model_name
        self.device = torch.device(device)

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        # On CUDA load in float16 to halve VRAM usage (~14 GB fp32 ‚Üí ~7 GB fp16).
        # On CPU keep float32 (fp16 is not accelerated on most CPUs).
        dtype = torch.float16 if self.device.type == "cuda" else torch.float32
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=dtype
        ).to(self.device)

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

    def _get_system_prompt(self, mode: str) -> str:
        """
        Return the system prompt for the given generation mode.

        Args:
            mode: ``"doctor"`` or ``"patient"``.

        Raises:
            ValueError: If *mode* is not recognized.
        """
        if mode not in self._SYSTEM_PROMPTS:
            raise ValueError(
                f"Unknown mode '{mode}'. Must be one of: "
                f"{list(self._SYSTEM_PROMPTS.keys())}"
            )
        return self._SYSTEM_PROMPTS[mode]

    def generate(
        self,
        patient_text: str,
        retrieved_sections: list[str],
        mode: str,
        max_new_tokens: int = 512,
    ) -> str:
        """
        Generate a clinical recommendation from guideline sections and patient data.

        The prompt is formatted with ``apply_chat_template`` to ensure proper
        instruction-following behaviour for chat-tuned models (e.g. Qwen2-Instruct).

        Args:
            patient_text: Text containing patient information.
            retrieved_sections: List of relevant guideline section texts.
            mode: Generation mode; ``"doctor"`` or ``"patient"``.
            max_new_tokens: Maximum number of new tokens to generate.

        Returns:
            Generated recommendation text (newly produced tokens only).
        """
        system_prompt = self._get_system_prompt(mode)
        retrieved_context = "\n\n---\n\n".join(retrieved_sections)

        user_message = (
            f"=== –ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ===\n"
            f"(–ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —ç—Ç–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã. –ö–∞–∂–¥—ã–π –ø–æ–º–µ—á–µ–Ω [–ö–†: –Ω–∞–∑–≤–∞–Ω–∏–µ].)\n\n"
            f"{retrieved_context}\n\n"
            f"=== –î–∞–Ω–Ω—ã–µ –ø–∞—Ü–∏–µ–Ω—Ç–∞ ===\n{patient_text}\n\n"
            f"=== –ù–ê–ü–û–ú–ò–ù–ê–ù–ò–ï ===\n"
            f"–ò—Å–ø–æ–ª—å–∑—É–π –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤—ã—à–µ. "
            f"–ù–µ –Ω–∞–∑—ã–≤–∞–π –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ [–ö–†: ...]. "
            f"–ü—Ä–∏ –∫–∞–∂–¥–æ–º —Ç–µ–∑–∏—Å–µ —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫: (–ö–†: –Ω–∞–∑–≤–∞–Ω–∏–µ)."
        )

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ]

        # apply_chat_template adds model-specific special tokens and roles.
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )

        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
        ).to(self.device)

        with torch.no_grad():
            output_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        # Decode only the newly generated tokens.
        input_length = inputs["input_ids"].shape[1]
        new_tokens = output_ids[0][input_length:]
        generated = self.tokenizer.decode(new_tokens, skip_special_tokens=True)
        return self._ensure_disclaimer(generated)

    def answer(
        self,
        question: str,
        patient_text: str,
        retrieved_sections: list[str],
        mode: str,
        max_new_tokens: int = 512,
    ) -> str:
        """
        Answer a follow-up question using the context from a previous analysis.

        Uses the same system prompt and retrieved sections as the main analysis,
        but appends the user's specific question to the user message.

        Args:
            question: The follow-up question from the user.
            patient_text: Original patient record text.
            retrieved_sections: Guideline sections retrieved during analysis.
            mode: ``"doctor"`` or ``"patient"``.
            max_new_tokens: Maximum number of new tokens to generate.

        Returns:
            Answer text with disclaimer guaranteed.
        """
        system_prompt = self._get_system_prompt(mode)
        retrieved_context = "\n\n---\n\n".join(retrieved_sections)

        user_message = (
            f"=== –ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ===\n"
            f"(–ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —ç—Ç–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã. –ö–∞–∂–¥—ã–π –ø–æ–º–µ—á–µ–Ω [–ö–†: –Ω–∞–∑–≤–∞–Ω–∏–µ].)\n\n"
            f"{retrieved_context}\n\n"
            f"=== –î–∞–Ω–Ω—ã–µ –ø–∞—Ü–∏–µ–Ω—Ç–∞ ===\n{patient_text}\n\n"
            f"=== –í–æ–ø—Ä–æ—Å ===\n{question}\n\n"
            f"=== –ù–ê–ü–û–ú–ò–ù–ê–ù–ò–ï ===\n"
            f"–ò—Å–ø–æ–ª—å–∑—É–π –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤—ã—à–µ. "
            f"–ù–µ –Ω–∞–∑—ã–≤–∞–π –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ [–ö–†: ...]. "
            f"–ü—Ä–∏ –∫–∞–∂–¥–æ–º —Ç–µ–∑–∏—Å–µ —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫: (–ö–†: –Ω–∞–∑–≤–∞–Ω–∏–µ)."
        )

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ]

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )

        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
        ).to(self.device)

        with torch.no_grad():
            output_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        input_length = inputs["input_ids"].shape[1]
        new_tokens = output_ids[0][input_length:]
        generated = self.tokenizer.decode(new_tokens, skip_special_tokens=True)
        return self._ensure_disclaimer(generated)

    def generate_structured(
        self,
        patient_text: str,
        main_analysis: str,
        max_new_tokens: int = 350,
    ) -> dict | None:
        """
        Extract structured sections from patient data and main analysis.

        Makes a second inference pass with a JSON-focused prompt.
        Returns a parsed dict or None if JSON extraction fails.

        Expected structure::

            {
              "diagnosis": "...",
              "age": "...",
              "comorbidities": "...",
              "overall_score": 75,
              "compliant": [
                {"title": "...", "category": "–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞|–¢–µ—Ä–∞–ø–∏—è|–ö–æ–Ω—Ç—Ä–æ–ª—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "text": "..."}
              ],
              "non_compliant": [
                {"title": "...", "category": "–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞|–¢–µ—Ä–∞–ø–∏—è|–ö–æ–Ω—Ç—Ä–æ–ª—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "text": "..."}
              ],
              "recommendations": [{"title": "...", "text": "..."}]
            }
        """
        system_prompt = (
            "–¢—ã ‚Äî —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. "
            "–ò–∑–≤–ª–µ–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ü–∏–µ–Ω—Ç–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞ –≤ —Ñ–æ—Ä–º–∞—Ç JSON. "
            "–í—ã–≤–µ–¥–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON-–æ–±—ä–µ–∫—Ç ‚Äî –±–µ–∑ markdown, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π.\n\n"
            "–¢—Ä–µ–±—É–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:\n"
            "{\n"
            '  "diagnosis": "–æ—Å–Ω–æ–≤–Ω–æ–π –¥–∏–∞–≥–Ω–æ–∑",\n'
            '  "age": "–≤–æ–∑—Ä–∞—Å—Ç –ø–∞—Ü–∏–µ–Ω—Ç–∞",\n'
            '  "comorbidities": "—Å–æ–ø—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞",\n'
            '  "overall_score": <—Ü–µ–ª–æ–µ —á–∏—Å–ª–æ 0-100>,\n'
            '  "compliant": [\n'
            '    {"title": "...", "category": "–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞|–¢–µ—Ä–∞–ø–∏—è|–ö–æ–Ω—Ç—Ä–æ–ª—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "text": "..."}\n'
            '  ],\n'
            '  "non_compliant": [\n'
            '    {"title": "...", "category": "–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞|–¢–µ—Ä–∞–ø–∏—è|–ö–æ–Ω—Ç—Ä–æ–ª—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "text": "..."}\n'
            '  ],\n'
            '  "recommendations": [\n'
            '    {"title": "...", "text": "..."}\n'
            '  ]\n'
            "}\n\n"
            "–ü—Ä–∞–≤–∏–ª–∞:\n"
            "- diagnosis, age, comorbidities: –∏–∑ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ü–∏–µ–Ω—Ç–∞\n"
            "- overall_score: % —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ª–µ—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º (0-100)\n"
            "- compliant: –ø—É–Ω–∫—Ç—ã –ª–µ—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –°–û–û–¢–í–ï–¢–°–¢–í–£–Æ–¢ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º (2-5 –ø—É–Ω–∫—Ç–æ–≤)\n"
            "- non_compliant: –ø—É–Ω–∫—Ç—ã –ª–µ—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –∏–ª–∏ –≤—ã–∑—ã–≤–∞—é—Ç –≤–æ–ø—Ä–æ—Å—ã (1-4 –ø—É–Ω–∫—Ç–∞)\n"
            "- category ‚Äî –æ–¥–Ω–æ –∏–∑: –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞, –¢–µ—Ä–∞–ø–∏—è, –ö–æ–Ω—Ç—Ä–æ–ª—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏\n"
            "- recommendations: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (1-3 –ø—É–Ω–∫—Ç–∞)\n"
            "- –í–µ—Å—å —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ\n"
        )

        user_message = (
            f"=== –î–∞–Ω–Ω—ã–µ –ø–∞—Ü–∏–µ–Ω—Ç–∞ ===\n{patient_text[:2000]}\n\n"
            f"=== –ê–Ω–∞–ª–∏–∑ ===\n{main_analysis[:3000]}"
        )

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ]

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )

        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
        ).to(self.device)

        with torch.no_grad():
            output_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        input_length = inputs["input_ids"].shape[1]
        new_tokens = output_ids[0][input_length:]
        generated = self.tokenizer.decode(new_tokens, skip_special_tokens=True)

        # Strip markdown code fences and extract the JSON object.
        cleaned = re.sub(r"```(?:json)?", "", generated).strip()
        start = cleaned.find("{")
        end = cleaned.rfind("}") + 1
        if start < 0 or end <= start:
            logger.warning("generate_structured: no JSON object found in output")
            return None
        try:
            return json.loads(cleaned[start:end])
        except json.JSONDecodeError as exc:
            logger.warning("generate_structured: JSON parse error: %s", exc)
            return None

    def _ensure_disclaimer(self, text: str) -> str:
        """
        Append the mandatory disclaimer if the model omitted it.

        The disclaimer is required by the system prompt, but may be truncated
        when max_new_tokens is reached before the model finishes generating.
        """
        if "–ù–µ —è–≤–ª—è–µ—Ç—Å—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–µ–π" not in text:
            return text + self._DISCLAIMER
        return text
