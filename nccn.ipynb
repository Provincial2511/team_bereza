{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5f92053-d39a-4e14-9405-35b1dab50494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbf8877-63cf-4efd-94c9-abebc9f05186",
   "metadata": {},
   "source": [
    "–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d20985db-e9be-420c-9aac-f56510112e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "NCCN Website Scraper\n",
    "Scrapes NCCN guideline category pages and generates YAML index documents\n",
    "Supports intelligent caching mechanism for MCP Server automation scenarios\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "import yaml\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa5d9fa-3413-4f5b-88e2-623c366d27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure logs directory exists\n",
    "LOGS_DIR = os.path.join(os.path.dirname(__file__), 'logs')\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "# Configure logging for this module specifically\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Only configure handlers if they haven't been added yet\n",
    "if not logger.handlers:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    # Add console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    # Add file handler\n",
    "    file_handler = logging.FileHandler(os.path.join(LOGS_DIR, 'nccn_get_index.log'))\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    # Prevent propagation to root logger to avoid duplicate logs\n",
    "    logger.propagate = False\n",
    "\n",
    "# Constants\n",
    "DEFAULT_OUTPUT_FILE = 'nccn_guidelines_index.yaml'\n",
    "CACHE_MAX_AGE_DAYS = 7  # Default maximum cache file validity period (days)\n",
    "\n",
    "\n",
    "async def fetch_page(client: httpx.AsyncClient, url: str, max_retries: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Asynchronously fetch single page content with retry mechanism\n",
    "    \n",
    "    Args:\n",
    "        client: httpx async client\n",
    "        url: URL to scrape\n",
    "        max_retries: Maximum retry attempts, default 3\n",
    "    \n",
    "    Returns:\n",
    "        Page HTML content\n",
    "    \"\"\"\n",
    "    import asyncio\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            response = await client.get(url, timeout=30.0)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries:\n",
    "                logger.warning(f\"Failed to fetch page {url} (attempt {attempt + 1}): {e}, retrying in 1 second...\")\n",
    "                await asyncio.sleep(1)  # Wait 1 second before retry\n",
    "            else:\n",
    "                logger.error(f\"Final failure to fetch page {url} (after {max_retries} retries): {e}\")\n",
    "                return \"\"\n",
    "\n",
    "\n",
    "async def get_page_title(client: httpx.AsyncClient, url: str) -> str:\n",
    "    \"\"\"\n",
    "    Get page title\n",
    "    \n",
    "    Args:\n",
    "        client: httpx async client\n",
    "        url: Page URL\n",
    "    \n",
    "    Returns:\n",
    "        Page title\n",
    "    \"\"\"\n",
    "    # Category pages are important\n",
    "    html = await fetch_page(client, url, max_retries=3)\n",
    "    if not html:\n",
    "        return \"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag:\n",
    "        return title_tag.get_text(strip=True)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "async def extract_item_links(client: httpx.AsyncClient, url: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract links and titles from div elements with class 'item-name' on the page\n",
    "    \n",
    "    Args:\n",
    "        client: httpx async client\n",
    "        url: Page URL\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing links and titles\n",
    "    \"\"\"\n",
    "    # Category pages are important, use more retries\n",
    "    html = await fetch_page(client, url, max_retries=5)\n",
    "    if not html:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    items = []\n",
    "    \n",
    "    # Find div elements with class 'item-name'\n",
    "    item_divs = soup.find_all('div', class_='item-name')\n",
    "    \n",
    "    for div in item_divs:\n",
    "        # Find link elements under the div\n",
    "        link_elem = div.find('a')\n",
    "        if link_elem:\n",
    "            href = link_elem.get('href')\n",
    "            title = link_elem.get_text(strip=True)\n",
    "            \n",
    "            if href and title:\n",
    "                # Convert to absolute URL\n",
    "                absolute_url = urljoin(url, href)\n",
    "                items.append({\n",
    "                    'title': title,\n",
    "                    'url': absolute_url\n",
    "                })\n",
    "    \n",
    "    return items\n",
    "\n",
    "\n",
    "async def find_nccn_guideline_link(client: httpx.AsyncClient, url: str) -> str:\n",
    "    \"\"\"\n",
    "    Find hyperlink of element containing 'NCCN guidelines' text on third-level page\n",
    "    \n",
    "    Args:\n",
    "        client: httpx async client\n",
    "        url: Third-level page URL\n",
    "    \n",
    "    Returns:\n",
    "        NCCN guidelines link, returns empty string if not found\n",
    "    \"\"\"\n",
    "    # Third-level pages are numerous\n",
    "    html = await fetch_page(client, url, max_retries=3)\n",
    "    if not html:\n",
    "        return \"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Find elements containing 'NCCN guidelines' text\n",
    "    for elem in soup.find_all(['a', 'span', 'div', 'p']):\n",
    "        text = elem.get_text(strip=True).lower()\n",
    "        if text == \"nccn guidelines\":\n",
    "            # If it's a link element, return href directly\n",
    "            if elem.name == 'a' and elem.get('href'):\n",
    "                return urljoin(url, elem.get('href'))\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "async def process_single_item(client: httpx.AsyncClient, item: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Process single item, find its NCCN guidelines link\n",
    "    \n",
    "    Args:\n",
    "        client: httpx async client\n",
    "        item: Dictionary containing title and url\n",
    "    \n",
    "    Returns:\n",
    "        Enhanced item dictionary containing guideline_link\n",
    "    \"\"\"\n",
    "    guideline_link = await find_nccn_guideline_link(client, item['url'])\n",
    "    return {\n",
    "        'title': item['title'],\n",
    "        'url': item['url'],\n",
    "        'guideline_link': guideline_link\n",
    "    }\n",
    "\n",
    "\n",
    "async def process_category(client: httpx.AsyncClient, category_num: int) -> dict:\n",
    "    \"\"\"\n",
    "    Process single category page\n",
    "    \n",
    "    Args:\n",
    "        client: httpx async client\n",
    "        category_num: Category number (1-4)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing category information and sub-items\n",
    "    \"\"\"\n",
    "    category_url = f\"https://www.nccn.org/guidelines/category_{category_num}\"\n",
    "    logger.info(f\"Processing category page: {category_url}\")\n",
    "    \n",
    "    # Get page title\n",
    "    title = await get_page_title(client, category_url)\n",
    "    \n",
    "    # Get item links from page\n",
    "    items = await extract_item_links(client, category_url)\n",
    "    \n",
    "    if not items:\n",
    "        logger.warning(f\"Category {category_num} found no items\")\n",
    "        return {\n",
    "            'category_num': category_num,\n",
    "            'title': title,\n",
    "            'url': category_url,\n",
    "            'items': []\n",
    "        }\n",
    "    \n",
    "    logger.info(f\"Category {category_num} found {len(items)} items, starting concurrent processing of third-level pages...\")\n",
    "    \n",
    "    # Process all third-level pages concurrently\n",
    "    tasks = [process_single_item(client, item) for item in items]\n",
    "    enhanced_items = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    # Filter out exception results, keep valid results\n",
    "    valid_items = []\n",
    "    for i, result in enumerate(enhanced_items):\n",
    "        if isinstance(result, Exception):\n",
    "            logger.error(f\"Failed to process item {items[i]['url']}: {result}\")\n",
    "            # Keep original information even if failed\n",
    "            valid_items.append({\n",
    "                'title': items[i]['title'],\n",
    "                'url': items[i]['url'],\n",
    "                'guideline_link': ''\n",
    "            })\n",
    "        else:\n",
    "            valid_items.append(result)\n",
    "    \n",
    "    logger.info(f\"Category {category_num} third-level page processing completed\")\n",
    "    \n",
    "    return {\n",
    "        'category_num': category_num,\n",
    "        'title': title,\n",
    "        'url': category_url,\n",
    "        'items': valid_items\n",
    "    }\n",
    "\n",
    "\n",
    "async def scrape_all_categories() -> list:\n",
    "    \"\"\"\n",
    "    Scrape all category pages\n",
    "    \n",
    "    Returns:\n",
    "        List of all category data\n",
    "    \"\"\"\n",
    "    async with httpx.AsyncClient(\n",
    "        headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        },\n",
    "        follow_redirects=True,\n",
    "        timeout=httpx.Timeout(30.0, connect=10.0),  # Set connection and read timeout\n",
    "        limits=httpx.Limits(max_keepalive_connections=10, max_connections=20)  # Limit connections\n",
    "    ) as client:\n",
    "        \n",
    "        # Process 4 category pages concurrently\n",
    "        tasks = [process_category(client, i) for i in range(1, 5)]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        # Filter out exception results\n",
    "        valid_results = [r for r in results if not isinstance(r, Exception)]\n",
    "        \n",
    "        return valid_results\n",
    "\n",
    "\n",
    "def generate_yaml(categories_data: list) -> str:\n",
    "    \"\"\"\n",
    "    Generate YAML format guideline index\n",
    "    \n",
    "    Args:\n",
    "        categories_data: List of category data\n",
    "    \n",
    "    Returns:\n",
    "        YAML format document string\n",
    "    \"\"\"\n",
    "    # Build hierarchical data structure\n",
    "    categories = []\n",
    "    \n",
    "    for category in categories_data:\n",
    "        category_title = category.get('title', f'Category {category[\"category_num\"]}')\n",
    "        \n",
    "        # Collect all valid guidelines under this category\n",
    "        guidelines = []\n",
    "        for item in category.get('items', []):\n",
    "            # Only keep items with guideline_link\n",
    "            if item.get('guideline_link'):\n",
    "                guidelines.append({\n",
    "                    'title': item['title'],\n",
    "                    'url': item['guideline_link']\n",
    "                })\n",
    "        \n",
    "        # Only add category if it has valid guidelines\n",
    "        if guidelines:\n",
    "            categories.append({\n",
    "                'category': category_title,\n",
    "                'guidelines': guidelines\n",
    "            })\n",
    "    \n",
    "    # Convert to YAML format\n",
    "    yaml_data = {\n",
    "        'nccn_guidelines': categories\n",
    "    }\n",
    "    \n",
    "    return yaml.dump(yaml_data, default_flow_style=False, allow_unicode=True, sort_keys=False)\n",
    "\n",
    "\n",
    "def check_cache_file(output_file: str = DEFAULT_OUTPUT_FILE) -> dict:\n",
    "    \"\"\"\n",
    "    Check cache file status\n",
    "    \n",
    "    Args:\n",
    "        output_file: Output file path\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing cache file information\n",
    "    \"\"\"\n",
    "    cache_info = {\n",
    "        'exists': False,\n",
    "        'file_path': output_file,\n",
    "        'size': 0,\n",
    "        'created_time': None,\n",
    "        'age_days': 0,\n",
    "        'is_valid': False\n",
    "    }\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        cache_info['exists'] = True\n",
    "        stat = os.stat(output_file)\n",
    "        cache_info['size'] = stat.st_size\n",
    "        cache_info['created_time'] = datetime.fromtimestamp(stat.st_mtime)\n",
    "        \n",
    "        # Calculate file age\n",
    "        age_delta = datetime.now() - cache_info['created_time']\n",
    "        cache_info['age_days'] = age_delta.days\n",
    "        \n",
    "        # Check if within validity period and file is not empty\n",
    "        cache_info['is_valid'] = cache_info['age_days'] < CACHE_MAX_AGE_DAYS and cache_info['size'] > 0\n",
    "    \n",
    "    return cache_info\n",
    "\n",
    "\n",
    "def load_cached_data(output_file: str = DEFAULT_OUTPUT_FILE) -> dict:\n",
    "    \"\"\"\n",
    "    Load cached YAML data\n",
    "    \n",
    "    Args:\n",
    "        output_file: Output file path\n",
    "    \n",
    "    Returns:\n",
    "        Parsed YAML data, returns empty dict if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            return yaml.safe_load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read cache file: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "async def ensure_nccn_index(output_file: str = DEFAULT_OUTPUT_FILE, max_age_days: int = CACHE_MAX_AGE_DAYS) -> dict:\n",
    "    \"\"\"\n",
    "    Ensure NCCN guideline index exists and is valid\n",
    "    This is the main interface for MCP Server calls\n",
    "    \n",
    "    Args:\n",
    "        output_file: Output file path\n",
    "        max_age_days: Maximum cache file validity period (days)\n",
    "    \n",
    "    Returns:\n",
    "        Parsed guideline index data\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Check cache file\n",
    "    cache_info = check_cache_file(output_file)\n",
    "    \n",
    "    # Determine if re-scraping is needed\n",
    "    should_scrape = not cache_info['exists'] or not cache_info['is_valid']\n",
    "    \n",
    "    if cache_info['exists']:\n",
    "        if cache_info['is_valid']:\n",
    "            logger.info(f\"Using valid cache file: {output_file} (created at {cache_info['created_time'].strftime('%Y-%m-%d %H:%M:%S')}, {cache_info['age_days']} days ago)\")\n",
    "        else:\n",
    "            logger.info(f\"Cache file expired ({cache_info['age_days']} days > {max_age_days} days) or corrupted, starting re-scraping...\")\n",
    "    else:\n",
    "        logger.info(\"Cache file not found, starting NCCN guideline index scraping...\")\n",
    "    \n",
    "    if should_scrape:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Scrape all category data\n",
    "            categories_data = await scrape_all_categories()\n",
    "            \n",
    "            if not categories_data:\n",
    "                logger.error(\"Scraping failed, no data retrieved\")\n",
    "                # If scraping fails but old cache exists, try using old cache\n",
    "                if cache_info['exists']:\n",
    "                    logger.info(\"Scraping failed, attempting to use existing cache file\")\n",
    "                    return load_cached_data(output_file)\n",
    "                return {}\n",
    "            \n",
    "            # Generate YAML document\n",
    "            yaml_content = generate_yaml(categories_data)\n",
    "            \n",
    "            # Save to file\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(yaml_content)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            total_guidelines = sum(len(cat.get('items', [])) for cat in categories_data)\n",
    "            successful_guidelines = sum(\n",
    "                len([item for item in cat.get('items', []) if item.get('guideline_link')])\n",
    "                for cat in categories_data\n",
    "            )\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            logger.info(f\"Scraping completed! Index saved to {output_file}\")\n",
    "            logger.info(f\"Processed {len(categories_data)} categories, found {successful_guidelines}/{total_guidelines} valid guideline links\")\n",
    "            logger.info(f\"Scraping time: {elapsed_time:.2f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during scraping process: {e}\")\n",
    "            # If scraping fails but cache exists, use cache\n",
    "            if cache_info['exists']:\n",
    "                logger.info(\"Scraping failed, using existing cache file\")\n",
    "                return load_cached_data(output_file)\n",
    "            return {}\n",
    "    \n",
    "    # Load and return data\n",
    "    cached_data = load_cached_data(output_file)\n",
    "    if cached_data and 'nccn_guidelines' in cached_data:\n",
    "        total_categories = len(cached_data['nccn_guidelines'])\n",
    "        total_guidelines = sum(len(cat.get('guidelines', [])) for cat in cached_data['nccn_guidelines'])\n",
    "        logger.info(f\"NCCN guideline index ready: {total_categories} categories, {total_guidelines} total guidelines\")\n",
    "    else:\n",
    "        logger.warning(\"Guideline index file format is abnormal\")\n",
    "    \n",
    "    return cached_data\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function - for direct script testing\n",
    "    \"\"\"\n",
    "    result = await ensure_nccn_index()\n",
    "    if result:\n",
    "        logger.info(\"Guideline index retrieved successfully\")\n",
    "    else:\n",
    "        logger.error(\"Failed to retrieve guideline index\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df95790c-cca2-4e26-a522-695dccef8f2c",
   "metadata": {},
   "source": [
    "##  —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –≤ –ø–∞–ø–∫—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12190a58-b78f-47c0-abd2-b6684bdf4f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "NCCN Automatic Login and PDF Downloader\n",
    "‚úÖ Fixed for Jupyter/Colab (no __file__ dependency)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# === Universal script directory (works in .py, Jupyter, Colab) ===\n",
    "SCRIPT_DIR = (\n",
    "    os.path.dirname(os.path.abspath(__file__))\n",
    "    if '__file__' in globals()\n",
    "    else os.getcwd()\n",
    ")\n",
    "\n",
    "# Ensure directories exist\n",
    "LOGS_DIR = os.path.join(SCRIPT_DIR, 'logs')\n",
    "PDFS_DIR = os.path.join(SCRIPT_DIR, 'pdfs')\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "os.makedirs(PDFS_DIR, exist_ok=True)\n",
    "\n",
    "# === Credentials ===\n",
    "EMAIL = \"#####\"\n",
    "PASSWORD = \"#####\"\n",
    "\n",
    "# === Logging configuration ===\n",
    "logger = logging.getLogger(__name__)\n",
    "if not logger.handlers:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Console handler\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    console.setFormatter(formatter)\n",
    "    logger.addHandler(console)\n",
    "\n",
    "    # File handler\n",
    "    file_handler = logging.FileHandler(os.path.join(LOGS_DIR, 'nccn_downloader.log'))\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    logger.propagate = False\n",
    "\n",
    "# Constants\n",
    "PDF_CACHE_MAX_AGE_DAYS = 7\n",
    "\n",
    "\n",
    "def check_pdf_cache_age(file_path: str, max_age_days: int = PDF_CACHE_MAX_AGE_DAYS) -> dict:\n",
    "    \"\"\"\n",
    "    Check PDF cache file age and validity.\n",
    "    \"\"\"\n",
    "    cache_info = {\n",
    "        'exists': False,\n",
    "        'file_path': file_path,\n",
    "        'size': 0,\n",
    "        'modified_time': None,\n",
    "        'age_days': 0,\n",
    "        'is_valid': False\n",
    "    }\n",
    "    if os.path.exists(file_path):\n",
    "        cache_info['exists'] = True\n",
    "        stat = os.stat(file_path)\n",
    "        cache_info['size'] = stat.st_size\n",
    "        cache_info['modified_time'] = datetime.fromtimestamp(stat.st_mtime)\n",
    "        age_delta = datetime.now() - cache_info['modified_time']\n",
    "        cache_info['age_days'] = age_delta.days\n",
    "        cache_info['is_valid'] = (cache_info['age_days'] < max_age_days) and (cache_info['size'] > 0)\n",
    "\n",
    "        logger.info(f\"PDF cache check: {file_path}\")\n",
    "        logger.info(f\"  - Size: {cache_info['size']} bytes\")\n",
    "        logger.info(f\"  - Age: {cache_info['age_days']} days\")\n",
    "        logger.info(f\"  - Valid: {cache_info['is_valid']}\")\n",
    "    return cache_info\n",
    "\n",
    "\n",
    "class NCCNDownloader:\n",
    "    def __init__(self, username=None, password=None):\n",
    "        \"\"\"\n",
    "        Initializes the NCCN Downloader.\n",
    "        \n",
    "        Args:\n",
    "            username (str, optional): Username (email address).\n",
    "            password (str, optional): Password.\n",
    "        \"\"\"\n",
    "        self.session = httpx.AsyncClient()\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        # Set request headers to simulate a browser visit\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        })\n",
    "        \n",
    "    async def login(self, username, password, target_url=\"https://www.nccn.org/professionals/physician_gls/pdf/all.pdf\"):\n",
    "        \"\"\"\n",
    "        Logs into the NCCN website.\n",
    "        \n",
    "        Args:\n",
    "            username (str): Username (email address).\n",
    "            password (str): Password.\n",
    "            target_url (str): The target URL to access after login.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if login is successful, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Accessing login page...\")\n",
    "            \n",
    "            # First, access the target URL, which will redirect to the login page\n",
    "            login_response = await self.session.get(target_url, follow_redirects=True)\n",
    "            \n",
    "            logger.info(f\"Login page response status: {login_response.status_code}\")\n",
    "            logger.info(f\"Login page final URL: {login_response.url}\")\n",
    "            \n",
    "            if login_response.status_code != 200:\n",
    "                logger.error(f\"Failed to access login page, status code: {login_response.status_code}\")\n",
    "                return False\n",
    "            \n",
    "            # Parse the login page\n",
    "            soup = BeautifulSoup(login_response.text, 'html.parser')\n",
    "            \n",
    "            # Find the login form\n",
    "            form = soup.find('form', {'action': '/login/Index/'})\n",
    "            if not form:\n",
    "                logger.error(\"Login form not found.\")\n",
    "                logger.debug(f\"Page content preview: {login_response.text[:1000]}...\")\n",
    "                return False\n",
    "            \n",
    "            # Extract hidden fields\n",
    "            hidden_inputs = form.find_all('input', {'type': 'hidden'})\n",
    "            form_data = {}\n",
    "            \n",
    "            for input_field in hidden_inputs:\n",
    "                name = input_field.get('name')\n",
    "                value = input_field.get('value', '')\n",
    "                if name:\n",
    "                    form_data[name] = value\n",
    "            \n",
    "            logger.info(f\"Found {len(form_data)} hidden form fields\")\n",
    "            \n",
    "            # Add login credentials\n",
    "            form_data.update({\n",
    "                'Username': username,\n",
    "                'Password': password,\n",
    "                'RememberMe': 'false',  # Do not remember by default\n",
    "            })\n",
    "            \n",
    "            logger.info(\"Submitting login information...\")\n",
    "            \n",
    "            # Submit the login form\n",
    "            login_url = \"https://www.nccn.org/login/\"\n",
    "            \n",
    "            # Set specific headers for the login request\n",
    "            login_headers = {\n",
    "                'Content-Type': 'application/x-www-form-urlencoded',\n",
    "                'Referer': str(login_response.url),\n",
    "                'Origin': 'https://www.nccn.org',\n",
    "            }\n",
    "            \n",
    "            login_result = await self.session.post(\n",
    "                login_url,\n",
    "                data=form_data,\n",
    "                headers=login_headers,\n",
    "                follow_redirects=True\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Login result status: {login_result.status_code}\")\n",
    "            logger.info(f\"Login result final URL: {login_result.url}\")\n",
    "            \n",
    "            # Check if login was successful\n",
    "            if login_result.status_code == 200:\n",
    "                # Check if still on the login page (indicates login failure)\n",
    "                if '/login' in str(login_result.url) or 'Log in' in login_result.text:\n",
    "                    logger.error(\"Login failed: Incorrect username or password.\")\n",
    "                    return False\n",
    "                else:\n",
    "                    logger.info(\"Login successful!\")\n",
    "                    return True\n",
    "            else:\n",
    "                logger.error(f\"Login request failed, status code: {login_result.status_code}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during login: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    async def download_pdf(self, pdf_url, download_dir=None, username=None, password=None, skip_if_exists=True, max_cache_age_days=PDF_CACHE_MAX_AGE_DAYS):\n",
    "        \"\"\"\n",
    "        Downloads a PDF file, automatically logging in if required.\n",
    "        \n",
    "        Args:\n",
    "            pdf_url (str): URL of the PDF file.\n",
    "            download_dir (str, optional): Directory to save the PDF. Defaults to current directory.\n",
    "            username (str, optional): Username (email address), required if not already logged in.\n",
    "            password (str, optional): Password, required if not already logged in.\n",
    "            skip_if_exists (bool): Whether to skip download if the file already exists. Defaults to True.\n",
    "            max_cache_age_days (int): Maximum cache file validity period (days). Defaults to PDF_CACHE_MAX_AGE_DAYS.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (success (bool), saved_filename (str))\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Automatically extract filename from URL\n",
    "            filename = os.path.basename(pdf_url)\n",
    "            if not filename or not filename.endswith('.pdf'):\n",
    "                filename = 'nccn_guideline.pdf'\n",
    "            \n",
    "            if download_dir:\n",
    "                os.makedirs(download_dir, exist_ok=True)\n",
    "            else:\n",
    "                download_dir = os.getcwd() # Use current working directory if not specified\n",
    "            \n",
    "            save_path = os.path.join(download_dir, filename)\n",
    "            \n",
    "            # Check if file already exists and is still valid (not too old)\n",
    "            if skip_if_exists:\n",
    "                cache_info = check_pdf_cache_age(save_path, max_cache_age_days)\n",
    "                if cache_info['exists']:\n",
    "                    if cache_info['is_valid']:\n",
    "                        logger.info(f\"Using valid cached PDF: {save_path}\")\n",
    "                        logger.info(f\"File size: {cache_info['size']} bytes, age: {cache_info['age_days']} days\")\n",
    "                        return True, filename\n",
    "                    else:\n",
    "                        logger.info(f\"PDF cache expired ({cache_info['age_days']} days > {max_cache_age_days} days) or corrupted, re-downloading...\")\n",
    "                else:\n",
    "                    logger.info(f\"PDF not found in cache, downloading: {save_path}\")\n",
    "            \n",
    "            logger.info(f\"Downloading PDF: {pdf_url}\")\n",
    "            \n",
    "            # Set request headers for PDF download\n",
    "            pdf_headers = {\n",
    "                'Accept': 'application/pdf,*/*',\n",
    "                'Referer': 'https://www.nccn.org/',\n",
    "            }\n",
    "            \n",
    "            # First, make a regular GET request to check the response\n",
    "            response = await self.session.get(pdf_url, headers=pdf_headers, follow_redirects=True)\n",
    "            \n",
    "            logger.info(f\"Response status: {response.status_code}\")\n",
    "            logger.info(f\"Final URL: {response.url}\")\n",
    "            \n",
    "            # Check if we were redirected to a login page\n",
    "            if response.status_code == 200:\n",
    "                content_type = response.headers.get('Content-Type', '')\n",
    "                logger.info(f\"Content-Type: {content_type}\")\n",
    "                \n",
    "                # Check if this is actually a PDF\n",
    "                if 'application/pdf' in content_type:\n",
    "                    # This is a PDF, save it directly\n",
    "                    with open(save_path, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                    \n",
    "                    file_size = os.path.getsize(save_path)\n",
    "                    logger.info(f\"PDF file saved to: {save_path}\")\n",
    "                    logger.info(f\"File size: {file_size} bytes\")\n",
    "                    return True, filename\n",
    "                \n",
    "                elif 'text/html' in content_type:\n",
    "                    # This is HTML, likely a login page\n",
    "                    response_text = response.text\n",
    "                    if 'login' in response_text.lower() or 'log in' in response_text.lower():\n",
    "                        logger.info(\"Login required detected, attempting automatic login...\")\n",
    "                        \n",
    "                        # If login credentials are provided, attempt to log in\n",
    "                        login_username = username or self.username\n",
    "                        login_password = password or self.password\n",
    "                        \n",
    "                        if login_username and login_password:\n",
    "                            if await self.login(login_username, login_password, pdf_url):\n",
    "                                logger.info(\"Login successful, re-downloading PDF...\")\n",
    "                                time.sleep(1)  # Wait for login state to stabilize\n",
    "                                # Recursive call, but do not pass login credentials to avoid infinite loop\n",
    "                                return await self.download_pdf(pdf_url, download_dir=download_dir, skip_if_exists=skip_if_exists, max_cache_age_days=max_cache_age_days)\n",
    "                            else:\n",
    "                                logger.error(\"Automatic login failed.\")\n",
    "                                return False, filename\n",
    "                        else:\n",
    "                            logger.error(\"Login required but username and password not provided.\")\n",
    "                            return False, filename\n",
    "                    else:\n",
    "                        logger.warning(\"Received HTML response but no login form detected.\")\n",
    "                        logger.debug(f\"Response preview: {response_text[:500]}...\")\n",
    "                        return False, filename\n",
    "                else:\n",
    "                    logger.warning(f\"Unexpected content type: {content_type}\")\n",
    "                    return False, filename\n",
    "            \n",
    "            elif response.status_code == 302:\n",
    "                # Handle redirect manually if needed\n",
    "                redirect_url = response.headers.get('Location')\n",
    "                logger.info(f\"Received redirect to: {redirect_url}\")\n",
    "                \n",
    "                # Check if redirect is to login page\n",
    "                if redirect_url and 'login' in redirect_url.lower():\n",
    "                    logger.info(\"Redirected to login page, attempting automatic login...\")\n",
    "                    \n",
    "                    login_username = username or self.username\n",
    "                    login_password = password or self.password\n",
    "                    \n",
    "                    if login_username and login_password:\n",
    "                        if await self.login(login_username, login_password, pdf_url):\n",
    "                            logger.info(\"Login successful, re-downloading PDF...\")\n",
    "                            time.sleep(1)\n",
    "                            return await self.download_pdf(pdf_url, download_dir=download_dir, skip_if_exists=skip_if_exists, max_cache_age_days=max_cache_age_days)\n",
    "                        else:\n",
    "                            logger.error(\"Automatic login failed.\")\n",
    "                            return False, filename\n",
    "                    else:\n",
    "                        logger.error(\"Login required but username and password not provided.\")\n",
    "                        return False, filename\n",
    "                else:\n",
    "                    logger.error(f\"Unexpected redirect to: {redirect_url}\")\n",
    "                    return False, filename\n",
    "            \n",
    "            else:\n",
    "                logger.error(f\"Download failed, status code: {response.status_code}\")\n",
    "                return False, filename\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during download: {str(e)}\")\n",
    "            return False, filename\n",
    "    \n",
    "    async def __aenter__(self):\n",
    "        \"\"\"Asynchronous context manager entry point.\"\"\"\n",
    "        return self\n",
    "    \n",
    "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Asynchronous context manager exit point.\"\"\"\n",
    "        await self.session.aclose()\n",
    "\n",
    "\n",
    "\n",
    "# === Main ===\n",
    "async def main():\n",
    "    # PDF URLs (examples)\n",
    "    pdf_urls = [\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/all.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/aml.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/ampullary.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/anal.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/appendiceal.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/nmsc.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/b-cell.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/btc.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/bladder.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/bone.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/breast.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/castleman.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/cns.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/cervical.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/cll.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/cml.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/colon.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/cutaneous_lymphomas.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/dfsp.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/esophageal.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/gastric.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/gist.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/gtn.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/hairy_cell.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/head-and-neck.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/hepatobiliary.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/hcc.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/histiocytic_neoplasms.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/hodgkins.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/kaposi.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/kidney.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/cutaneous_melanoma.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/uveal.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/mcc.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/meso_peritoneal.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/meso_pleural.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/myeloma.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/mds.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/mlne.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/mpn.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/neuroblastoma.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/neuroendocrine.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/nscl.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/occult.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/ovarian.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/pancreatic.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/ped_all.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/ped_b-cell.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/ped_cns.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/ped_hodgkin.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/ped_sts.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/penile.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/prostate.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/rectal.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/small_bowel.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/sclc.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/sarcoma.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/squamous.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/amyloidosis.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/mastocytosis.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/t-cell.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/testicular.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/thymic.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/thyroid.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/uterine.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/vaginal.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/vulvar.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/waldenstroms.pdf\",\n",
    "    \"https://www.nccn.org/professionals/physician_gls/pdf/wilms_tumor.pdf\"\n",
    "]\n",
    "\n",
    "    async with NCCNDownloader() as downloader:\n",
    "        # Explicit login (optional ‚Äî will be auto-tried if needed)\n",
    "        # if not await downloader.login():\n",
    "        #     logger.error(\"‚ùå Login failed ‚Äî aborting.\")\n",
    "        #     return\n",
    "\n",
    "        for url in pdf_urls:\n",
    "            ok, filename = await downloader.download_pdf(\n",
    "                url,\n",
    "                username=EMAIL, password=PASSWORD,\n",
    "                download_dir=PDFS_DIR,\n",
    "                skip_if_exists=True\n",
    "            )\n",
    "            status = \"‚úÖ\" if ok else \"‚ùå\"\n",
    "            logger.info(f\"{status} {filename}\")\n",
    "\n",
    "        logger.info(\"üéâ All downloads completed.\")\n",
    "\n",
    "\n",
    "# === Run safely for both .py and Jupyter/Colab ===\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Regular Python script\n",
    "        asyncio.run(main())\n",
    "    except RuntimeError as e:\n",
    "        # \"asyncio.run() cannot be called from a running event loop\" ‚Äî Jupyter/Colab\n",
    "        logger.warning(\"Running in interactive mode ‚Üí using nest_asyncio fallback\")\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            loop.run_until_complete(main())\n",
    "        else:\n",
    "            asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431ab32-9b94-42e4-9b2d-245bb46e7b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
